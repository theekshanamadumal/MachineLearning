Lightning makes coding complex networks simple.
built on pure PyTorch
* Run your code on any hardware
* Performance & bottleneck profiler
* Model checkpointing
* 16-bit precision
* Run distributed training
* Logging
* Metrics
* Visualization
* checkpointing
* Early stopping

*  Without Lightning, the PyTorch code is allowed to be in arbitrary parts. With Lightning, this is structured.

*  As the project grows in complexity, your code wonâ€™t because Lightning abstracts out most of it.

*  You retain the flexibility of PyTorch because you have full control over the key points in training.

*  In Lightning you got a bunch of freebies such as a  progress bar

 ![image](https://github.com/theekshanamadumal/MachineLearning/assets/66960247/d058b302-56f9-4427-97be-8aa445e28909)
 


*  you also got a beautiful weights summary

 ![image](https://github.com/theekshanamadumal/MachineLearning/assets/66960247/0edfeda4-4fce-4404-a871-04bd0f98a591)

 

*  tensorboard logs



*  Lightning is known best for out of the box goodies such as TPU training

*  In Lightning, you can train your model on CPUs, GPUs, Multiple GPUs, or TPUs without changing a single line of your PyTorch code.

*  built in profiler that can tell you where the bottlenecks are in your training.

*  can also train on multiple GPUs at once without you doing any work (you still have to submit a SLURM job)


